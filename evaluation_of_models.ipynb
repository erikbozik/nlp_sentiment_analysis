{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6029dff-7ae3-46a6-8a37-73228f17a5ea",
   "metadata": {},
   "source": [
    "# Evaluation of DistilBERT and LSTM models for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648e77e-4be5-4877-96cc-f3cca747abfa",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe285b46-9d6b-498b-b9e3-82a5805ddd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebozik/anaconda3/envs/nlp2/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "2024-06-25 00:29:34.109473: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-25 00:29:34.136820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 00:29:34.927981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "from transformers import BertTokenizer\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DistilBertTokenizerFast\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb9046-b808-4360-8832-224ddebbc20f",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e33535-3ca4-4886-ad36-5f9017bf18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tweets_df = pd.read_csv('data/translated_tweets.csv.gz', compression='gzip')\n",
    "heureka_reviews_df = pd.read_json('data/reviews.json.gz', compression='gzip')\n",
    "gpt35_reviews_df = pd.read_csv('data/gpt_3.5_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed6258-9053-4414-9725-623aa7eba02e",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d683aa6-1f95-4714-8d3d-8141465c26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tweets_df = translated_tweets_df[['Sentiment', 'SentimentText']]\n",
    "translated_tweets_df.rename(columns={'Sentiment': 'labels', 'SentimentText': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be59c42b-3002-4b9e-a95d-b7339ec8e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heureka_reviews_df.dropna(inplace=True)\n",
    "heureka_reviews_df.loc[:, 'review_text'] = heureka_reviews_df['review_text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "heureka_reviews_df = heureka_reviews_df.loc[heureka_reviews_df[\"review_text\"] != '']\n",
    "heureka_reviews_df.loc[:, \"sentiment\"] = heureka_reviews_df[\"sentiment\"].astype('int8')\n",
    "heureka_reviews_df.reset_index(inplace=True, drop=True)\n",
    "heureka_reviews_df.rename(columns={'sentiment': 'labels', 'review_text': 'text'}, inplace=True) \n",
    "heureka_reviews_df['labels'] = heureka_reviews_df['labels'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947f7496-d11a-4d9b-b75d-08602fb1f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_reviews_df.rename(columns={'sentiment': 'labels', 'review_text': 'text'}, inplace=True)\n",
    "gpt35_reviews_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc8bd8a-b442-4d87-ba66-f161d17f8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tweets_df[\"data_source\"] = 1\n",
    "heureka_reviews_df[\"data_source\"] = 2\n",
    "gpt35_reviews_df[\"data_source\"] = 3\n",
    "# Combine the text and labels from each dataframe\n",
    "combined_reviews_df = pd.concat([\n",
    "    translated_tweets_df.sample(50000, random_state=42),\n",
    "    heureka_reviews_df.sample(50000, random_state=42),\n",
    "    gpt35_reviews_df.sample(50000, random_state=42)\n",
    "], ignore_index=True)\n",
    "\n",
    "del translated_tweets_df, heureka_reviews_df, gpt35_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6ac581",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews_df.drop(\"data_source\", axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a7d2a-1294-4421-b124-e4728faf4097",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56ba6a-a8d4-49d8-97bd-31cb05d5b5a4",
   "metadata": {},
   "source": [
    "### Functions for testing DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7dade7-192f-4df4-8dfb-cd78fea6cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Hugging Face Dataset and tokenize\n",
    "def df_to_dataset(df, tokenizer):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return dataset.map(tokenize_function, batched=True, num_proc=4)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_predictions), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868efa0-ebf5-420c-a980-682310bd1f38",
   "metadata": {},
   "source": [
    "### Test distilbert trained on gpt 3.5 generated reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac73b027-c6b8-42cc-bf45-9fcd70ebc0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f50282d18e4da4b85d3929fdefea76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebozik/anaconda3/envs/nlp2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6922a2b9504440abddd1579eccdad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./data/gpt35_reviews_best_model and using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c497ccdec44b0c80acf87b50903fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 9375/9375 [24:00<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------+\n",
      "|    Info    |              Details               |\n",
      "+------------+------------------------------------+\n",
      "|   Model    | distilbert-base-multilingual-cased |\n",
      "| Trained on |           gpt35_reviews            |\n",
      "| Tested on  |          combined_reviews          |\n",
      "|  Accuracy  |              0.80148               |\n",
      "|  F1 Score  |         0.8025904435336819         |\n",
      "| Precision  |         0.8057772363657557         |\n",
      "+------------+------------------------------------+\n",
      "Predictions: [1 0 0 ... 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved model\n",
    "model_dir = \"./data/gpt35_reviews_best_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "print(f\"Model loaded from {model_dir} and using device: {model.device}\")\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "# train_df, test_df = train_test_split(combined_reviews_df, test_size=0.05, stratify=combined_reviews_df['labels'], random_state=42)\n",
    "test_df = combined_reviews_df.copy()\n",
    "\n",
    "# train_dataset = df_to_dataset(train_df, tokenizer)\n",
    "test_dataset = df_to_dataset(test_df, tokenizer)\n",
    "\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader for the new dataset\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Perform evaluation\n",
    "predictions, labels = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Compute metrics\n",
    "precision, _, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"distilbert-base-multilingual-cased\"])\n",
    "table.add_row([\"Trained on\", \"gpt35_reviews\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e607c3f-ca6f-459e-b75d-9bf94360afd3",
   "metadata": {},
   "source": [
    "### Test distilbert model trained on translated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55fe8435-2963-48d5-8c4c-3db9480ef9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded from ./data/translated_tweets_best_model and using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ec48aa8eb34ba199a7d0bec9260877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 9375/9375 [23:51<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------+\n",
      "|    Info    |              Details               |\n",
      "+------------+------------------------------------+\n",
      "|   Model    | distilbert-base-multilingual-cased |\n",
      "| Trained on |         translated_tweets          |\n",
      "| Tested on  |          combined_reviews          |\n",
      "|  Accuracy  |              0.85208               |\n",
      "|  F1 Score  |         0.8521926841547429         |\n",
      "| Precision  |         0.8523332087639519         |\n",
      "+------------+------------------------------------+\n",
      "Predictions: [0 0 0 ... 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved model\n",
    "model_dir = \"./data/translated_tweets_best_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "print(f\"Model loaded from {model_dir} and using device: {model.device}\")\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "# train_df, test_df = train_test_split(combined_reviews_df, test_size=0.05, stratify=combined_reviews_df['labels'], random_state=42)\n",
    "test_df = combined_reviews_df.copy()\n",
    "\n",
    "# train_dataset = df_to_dataset(train_df, tokenizer)\n",
    "test_dataset = df_to_dataset(test_df, tokenizer)\n",
    "\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader for the new dataset\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Perform evaluation\n",
    "predictions, labels = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Compute metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"distilbert-base-multilingual-cased\"])\n",
    "table.add_row([\"Trained on\", \"translated_tweets\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5238e5-43f1-4b0b-a9ea-50e3f371886c",
   "metadata": {},
   "source": [
    "### Test DistilBERT model trained on heureka reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e5b6d2-cf15-46bf-90c0-7ff875396e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded from ./data/heureka_reviews_best_model and using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730cdc0845ff4315b5812aa9d81658f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 9375/9375 [23:44<00:00,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------+\n",
      "|    Info    |              Details               |\n",
      "+------------+------------------------------------+\n",
      "|   Model    | distilbert-base-multilingual-cased |\n",
      "| Trained on |          heureka_reviews           |\n",
      "| Tested on  |          combined_reviews          |\n",
      "|  Accuracy  |         0.8322866666666666         |\n",
      "|  F1 Score  |         0.8331767560724154         |\n",
      "| Precision  |         0.835950476236238          |\n",
      "+------------+------------------------------------+\n",
      "Predictions: [0 0 1 ... 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved model\n",
    "model_dir = \"./data/heureka_reviews_best_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "print(f\"Model loaded from {model_dir} and using device: {model.device}\")\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "# train_df, test_df = train_test_split(combined_reviews_df, test_size=0.05, stratify=combined_reviews_df['labels'], random_state=42)\n",
    "test_df = combined_reviews_df.copy()\n",
    "\n",
    "# train_dataset = df_to_dataset(train_df, tokenizer)\n",
    "test_dataset = df_to_dataset(test_df, tokenizer)\n",
    "\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader for the new dataset\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Perform evaluation\n",
    "predictions, labels = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Compute metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"distilbert-base-multilingual-cased\"])\n",
    "table.add_row([\"Trained on\", \"heureka_reviews\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95b486-d294-4fba-b86a-ea0f3318a2ee",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980a62a-7f7a-4d43-9944-179de01f2126",
   "metadata": {},
   "source": [
    "### Function for evaluating LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a39eee42-427e-4012-88d2-4cbfc8ea1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, X_test, Y_test):\n",
    "    # with tf.device('/CPU:0'):\n",
    "    predictions = model.predict(X_test, batch_size=1024)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(Y_test, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dec53-b4cc-4689-b5e2-cf9952407a0e",
   "metadata": {},
   "source": [
    "### Test LSTM model trained on gpt 3.5 generated reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33e2b728-f78e-4f27-9bda-ed30968c55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 01:41:43.859804: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.861157: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.861178: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.862897: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.862916: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.862924: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.863234: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.863263: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.863269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-25 01:41:43.863287: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-25 01:41:43.863307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8579 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 3s/step\n",
      "+------------+--------------------+\n",
      "|    Info    |      Details       |\n",
      "+------------+--------------------+\n",
      "|   Model    |        LSTM        |\n",
      "| Trained on |   gpt35_reviews    |\n",
      "| Tested on  |  combined_reviews  |\n",
      "|  Accuracy  | 0.7624266666666667 |\n",
      "|  F1 Score  | 0.7641939264390328 |\n",
      "| Precision  | 0.7925670080512627 |\n",
      "+------------+--------------------+\n",
      "Predictions: [0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "with open(\"lstm_gpt_tokenizer.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Maximum number of features\n",
    "max_features = 5000\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Prepare the data\n",
    "X = tokenizer.texts_to_sequences(combined_reviews_df['text'].values)\n",
    "X = pad_sequences(X, maxlen=max_features)\n",
    "Y = combined_reviews_df['labels'].values\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Load the LSTM model\n",
    "model = tf.keras.models.load_model('lstm_gpt.keras')\n",
    "\n",
    "# Perform evaluation\n",
    "accuracy, precision, recall, f1 = evaluate(model, X, Y)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# # Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"LSTM\"])\n",
    "table.add_row([\"Trained on\", \"gpt35_reviews\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f8fbf-a0d0-4525-af5a-999119e50ddc",
   "metadata": {},
   "source": [
    "### Test LSTM model trained on translated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c3888e1-720a-42bf-8126-1485158a3372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebozik/anaconda3/envs/nlp2/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 3s/step\n",
      "+------------+--------------------+\n",
      "|    Info    |      Details       |\n",
      "+------------+--------------------+\n",
      "|   Model    |        LSTM        |\n",
      "| Trained on | translated_tweets  |\n",
      "| Tested on  |  combined_reviews  |\n",
      "|  Accuracy  | 0.7824066666666667 |\n",
      "|  F1 Score  | 0.782297390342308  |\n",
      "| Precision  | 0.7821983792996207 |\n",
      "+------------+--------------------+\n",
      "Predictions: [0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "with open(\"tweets_lstm_tokenizer.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Maximum number of features\n",
    "max_features = 5000\n",
    "\n",
    "# Prepare the data\n",
    "X = tokenizer.texts_to_sequences(combined_reviews_df['text'].values)\n",
    "X = pad_sequences(X, maxlen=max_features)\n",
    "Y = combined_reviews_df['labels'].values\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n",
    "\n",
    "# # Load the LSTM model\n",
    "model = tf.keras.models.load_model('trained_lstm.keras')\n",
    "\n",
    "# Perform evaluation\n",
    "accuracy, precision, recall, f1 = evaluate(model, X, Y)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# # Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"LSTM\"])\n",
    "table.add_row([\"Trained on\", \"translated_tweets\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944444f-d3ef-4702-9629-8fd4d0186c15",
   "metadata": {},
   "source": [
    "### Test LSTM model trained on heureka reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e20d8a-ca9d-4cfb-97fd-bc15639c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 3s/step\n",
      "+------------+--------------------+\n",
      "|    Info    |      Details       |\n",
      "+------------+--------------------+\n",
      "|   Model    |        LSTM        |\n",
      "| Trained on |  heureka_reviews   |\n",
      "| Tested on  |  combined_reviews  |\n",
      "|  Accuracy  | 0.8210066666666667 |\n",
      "|  F1 Score  | 0.8211688727113685 |\n",
      "| Precision  | 0.8213692507387974 |\n",
      "+------------+--------------------+\n",
      "Predictions: [0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "with open(\"lstm_heureka_tokenizer.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Maximum number of features\n",
    "max_features = 5000\n",
    "\n",
    "# Prepare the data\n",
    "X = tokenizer.texts_to_sequences(combined_reviews_df['text'].values)\n",
    "X = pad_sequences(X, maxlen=max_features)\n",
    "Y = combined_reviews_df['labels'].values\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n",
    "\n",
    "# # Load the LSTM model\n",
    "model = tf.keras.models.load_model('lstm_heaureka.keras')\n",
    "\n",
    "# Perform evaluation\n",
    "accuracy, precision, recall, f1 = evaluate(model, X, Y)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"LSTM\"])\n",
    "table.add_row([\"Trained on\", \"heureka_reviews\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
