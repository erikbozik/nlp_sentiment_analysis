{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6029dff-7ae3-46a6-8a37-73228f17a5ea",
   "metadata": {},
   "source": [
    "# Evaluation of DistilBERT and LSTM models for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648e77e-4be5-4877-96cc-f3cca747abfa",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe285b46-9d6b-498b-b9e3-82a5805ddd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "from transformers import BertTokenizer\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DistilBertTokenizerFast\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb9046-b808-4360-8832-224ddebbc20f",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e33535-3ca4-4886-ad36-5f9017bf18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tweets_df = pd.read_csv('data/translated_tweets.csv.gz', compression='gzip')\n",
    "heureka_reviews_df = pd.read_json('data/reviews.json.gz', compression='gzip')\n",
    "gpt35_reviews_df = pd.read_csv('data/gpt_3.5_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed6258-9053-4414-9725-623aa7eba02e",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d683aa6-1f95-4714-8d3d-8141465c26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tweets_df = translated_tweets_df[['Sentiment', 'SentimentText']]\n",
    "translated_tweets_df.rename(columns={'Sentiment': 'labels', 'SentimentText': 'text'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be59c42b-3002-4b9e-a95d-b7339ec8e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heureka_reviews_df.dropna(inplace=True)\n",
    "heureka_reviews_df.loc[:, 'review_text'] = heureka_reviews_df['review_text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "heureka_reviews_df = heureka_reviews_df.loc[heureka_reviews_df[\"review_text\"] != '']\n",
    "heureka_reviews_df.loc[:, \"sentiment\"] = heureka_reviews_df[\"sentiment\"].astype('int8')\n",
    "heureka_reviews_df.reset_index(inplace=True, drop=True)\n",
    "heureka_reviews_df.rename(columns={'sentiment': 'labels', 'review_text': 'text'}, inplace=True) \n",
    "heureka_reviews_df['labels'] = heureka_reviews_df['labels'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947f7496-d11a-4d9b-b75d-08602fb1f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_reviews_df.rename(columns={'sentiment': 'labels', 'review_text': 'text'}, inplace=True)\n",
    "gpt35_reviews_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc8bd8a-b442-4d87-ba66-f161d17f8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the text and labels from each dataframe\n",
    "combined_reviews_df = pd.concat([\n",
    "    translated_tweets_df[['text', 'labels']],\n",
    "    heureka_reviews_df[['text', 'labels']],\n",
    "    gpt35_reviews_df[['text', 'labels']]\n",
    "], ignore_index=True)\n",
    "\n",
    "del translated_tweets_df, heureka_reviews_df, gpt35_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a7d2a-1294-4421-b124-e4728faf4097",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56ba6a-a8d4-49d8-97bd-31cb05d5b5a4",
   "metadata": {},
   "source": [
    "### Functions for testing DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e7dade7-192f-4df4-8dfb-cd78fea6cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Hugging Face Dataset and tokenize\n",
    "def df_to_dataset(df, tokenizer):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return dataset.map(tokenize_function, batched=True, num_proc=4)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_predictions), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868efa0-ebf5-420c-a980-682310bd1f38",
   "metadata": {},
   "source": [
    "### Test distilbert trained on gpt 3.5 generated reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac73b027-c6b8-42cc-bf45-9fcd70ebc0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./data/gpt35_reviews_best_model and using device: mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e79f61d584442097267bb843bcf639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3104 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 194/194 [01:04<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------+\n",
      "|    Info   |              Details               |\n",
      "+-----------+------------------------------------+\n",
      "|   Model   | distilbert-base-multilingual-cased |\n",
      "| Tested on |      combined_reviews (0.001)      |\n",
      "|  Accuracy |         0.7419458762886598         |\n",
      "|  F1 Score |         0.746779761550987          |\n",
      "| Precision |         0.7589421293817251         |\n",
      "|   Recall  |         0.7419458762886598         |\n",
      "+-----------+------------------------------------+\n",
      "Predictions: [0 1 1 ... 1 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved model\n",
    "model_dir = \"./data/gpt35_reviews_best_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "print(f\"Model loaded from {model_dir} and using device: {model.device}\")\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "train_df, test_df = train_test_split(combined_reviews_df, test_size=0.05, stratify=combined_reviews_df['labels'], random_state=42)\n",
    "\n",
    "# train_dataset = df_to_dataset(train_df, tokenizer)\n",
    "test_dataset = df_to_dataset(test_df, tokenizer)\n",
    "\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader for the new dataset\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Perform evaluation\n",
    "predictions, labels = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Compute metrics\n",
    "precision, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"distilbert-base-multilingual-cased\"])\n",
    "table.add_row([\"Trained on\", \"combined_reviews\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews (0.05)\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e607c3f-ca6f-459e-b75d-9bf94360afd3",
   "metadata": {},
   "source": [
    "### Test distilbert model trained on translated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe8435-2963-48d5-8c4c-3db9480ef9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model and data info\n",
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "data_name = \"gpt35_reviews_df\"\n",
    "\n",
    "# Load the saved model\n",
    "model_dir = \"./data/translated_tweets_best_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "print(f\"Model loaded from {model_dir} and using device: {model.device}\")\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "train_df, test_df = train_test_split(combined_reviews_df, test_size=0.05, stratify=combined_reviews_df['labels'], random_state=42)\n",
    "\n",
    "# train_dataset = df_to_dataset(train_df, tokenizer)\n",
    "test_dataset = df_to_dataset(test_df, tokenizer)\n",
    "\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader for the new dataset\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Perform evaluation\n",
    "predictions, labels = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Compute metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"distilbert-base-multilingual-cased\"])\n",
    "table.add_row([\"Trained on\", \"translated_tweets\"])\n",
    "table.add_row([\"Tested on\", \"translated_tweets (0.05)\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5238e5-43f1-4b0b-a9ea-50e3f371886c",
   "metadata": {},
   "source": [
    "### Test DistilBERT model trained on heureka reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5b6d2-cf15-46bf-90c0-7ff875396e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved model\n",
    "model_dir = \"./data/translated_tweets_best_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "print(f\"Model loaded from {model_dir} and using device: {model.device}\")\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "train_df, test_df = train_test_split(combined_reviews_df, test_size=0.05, stratify=combined_reviews_df['labels'], random_state=42)\n",
    "\n",
    "# train_dataset = df_to_dataset(train_df, tokenizer)\n",
    "test_dataset = df_to_dataset(test_df, tokenizer)\n",
    "\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader for the new dataset\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Perform evaluation\n",
    "predictions, labels = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Compute metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"distilbert-base-multilingual-cased\"])\n",
    "table.add_row([\"Trained on\", \"heureka_reviews\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews (0.05)\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95b486-d294-4fba-b86a-ea0f3318a2ee",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980a62a-7f7a-4d43-9944-179de01f2126",
   "metadata": {},
   "source": [
    "### Function for evaluating LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39eee42-427e-4012-88d2-4cbfc8ea1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, X_test, Y_test):\n",
    "    predictions = model.predict(X_test, batch_size=16)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(Y_test, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dec53-b4cc-4689-b5e2-cf9952407a0e",
   "metadata": {},
   "source": [
    "### Test LSTM model trained on gpt 3.5 generated reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2b728-f78e-4f27-9bda-ed30968c55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 21:43:40.624178: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-06-24 21:43:40.624356: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-06-24 21:43:40.624376: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-06-24 21:43:40.624424: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-24 21:43:40.624468: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "with open(\"lstm_gpt_tokenizer.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Maximum number of features\n",
    "max_features = 5000\n",
    "\n",
    "# Prepare the data\n",
    "X = tokenizer.texts_to_sequences(combined_reviews_df['text'].values)\n",
    "X = pad_sequences(X, maxlen=max_features)\n",
    "Y = combined_reviews_df['labels'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n",
    "\n",
    "# # Load the LSTM model\n",
    "model = tf.keras.models.load_model('lstm_gpt.keras')\n",
    "\n",
    "Perform evaluation\n",
    "accuracy, precision, recall, f1 = evaluate(model, X_test, Y_test)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"LSTM\"])\n",
    "table.add_row([\"Trained on\", \"gpt35_reviews\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews (0.05)\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f8fbf-a0d0-4525-af5a-999119e50ddc",
   "metadata": {},
   "source": [
    "### Test LSTM model trained on translated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3888e1-720a-42bf-8126-1485158a3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "with open(\"tweets_lstm_tokenizer.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Maximum number of features\n",
    "max_features = 5000\n",
    "\n",
    "# Prepare the data\n",
    "X = tokenizer.texts_to_sequences(combined_reviews_df['text'].values)\n",
    "X = pad_sequences(X, maxlen=max_features)\n",
    "Y = combined_reviews_df['labels'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n",
    "\n",
    "# # Load the LSTM model\n",
    "model = tf.keras.models.load_model('trained_lstm.keras')\n",
    "\n",
    "Perform evaluation\n",
    "accuracy, precision, recall, f1 = evaluate(model, X_test, Y_test)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"LSTM\"])\n",
    "table.add_row([\"Trained on\", \"translated_tweets\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews_df (0.05)\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944444f-d3ef-4702-9629-8fd4d0186c15",
   "metadata": {},
   "source": [
    "### Test LSTM model trained on heureka reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e20d8a-ca9d-4cfb-97fd-bc15639c6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "with open(\"lstm_heureka_tokenizer.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Maximum number of features\n",
    "max_features = 5000\n",
    "\n",
    "# Prepare the data\n",
    "X = tokenizer.texts_to_sequences(combined_reviews_df['text'].values)\n",
    "X = pad_sequences(X, maxlen=max_features)\n",
    "Y = combined_reviews_df['labels'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)\n",
    "\n",
    "# # Load the LSTM model\n",
    "model = tf.keras.models.load_model('lstm_heaureka.keras')\n",
    "\n",
    "Perform evaluation\n",
    "accuracy, precision, recall, f1 = evaluate(model, X_test, Y_test)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision'],\n",
    "    'Value': [accuracy, f1, precision]\n",
    "})\n",
    "\n",
    "# Print the results in a nice table format\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Info\", \"Details\"]\n",
    "table.add_row([\"Model\", \"LSTM\"])\n",
    "table.add_row([\"Trained on\", \"heureka_reviews\"])\n",
    "table.add_row([\"Tested on\", \"combined_reviews (0.05)\"])\n",
    "for index, row in results_df.iterrows():\n",
    "    table.add_row([row['Metric'], row['Value']])\n",
    "\n",
    "print(table)\n",
    "\n",
    "# If you just want to print predictions:\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
